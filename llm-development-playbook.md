# LLM Development Playbook
> Prompt library and workflow reference for LLM-augmented development
> Last updated: December 2024

---

## How to Use This Document

**Audience:** This document is for LLMs generating prompts. For human reference (emergencies, escalation, monitoring tasks), see [orchestrator-handbook.md](orchestrator-handbook.md).

**When asked to generate a prompt:** 
1. Find the relevant prompt in the [Prompts Library](#prompts-library)
2. Get variable values from [session-variables.md](session-variables.md)
3. Substitute variables and output the prompt

**When asked "what's next?":** Consult the [Workflow Phases](#workflow-phases) section.

**Context Persistence:** This playbook assumes [Project Ontos](https://github.com/ohjona/Project-Ontos) handles context sync between agents.

---

## Quick Navigation

### Roles
[Strategist](#-strategist) ¬∑ [Product Architect](#-product-architect) ¬∑ [Chief Architect](#-chief-architect) ¬∑ [Implementation Architect](#-implementation-architect) ¬∑ [Developer](#-developer-agentic) ¬∑ [Code Reviewer](#-code-reviewer) ¬∑ [Adversarial Reviewer](#-adversarial-reviewer-critical-role) ¬∑ [Synthesizer](#-synthesizer) ¬∑ [Debugger](#-debugger) ¬∑ [QA Analyst](#-qa-analyst)

### Core Prompts (in order of typical use)
1. [Implementation Architect Prompt](#implementation-architect-prompt) ‚Äî write spec
2. [Implementation Plan Review Prompt](#implementation-plan-review-prompt-fellow-architects) ‚Äî fellow architects review
3. [Implementation Architect Response Prompt](#implementation-architect-response-to-review-prompt) ‚Äî address feedback
4. [Fellow Architects Re-Review Prompt](#fellow-architects-re-review-prompt) ‚Äî check revisions
5. [Spec Finalization Prompt](#spec-finalization-prompt-after-convergence) ‚Äî clean up for developer
6. [Development Initiation Prompt](#development-initiation-prompt) ‚Äî start coding
7. [Code Reviewer Prompt](#code-reviewer-prompt) ‚Äî review PR
8. [Adversarial Reviewer Prompt](#adversarial-reviewer-prompt) ‚Äî try to break it
9. [Developer Response Prompt](#developer-response-to-review-prompt) ‚Äî address review feedback
10. [Code Reviewer Re-Review Prompt](#code-reviewer-re-review-prompt) ‚Äî verify fixes
11. [Synthesizer Prompt](#synthesizer-prompt) ‚Äî compress for decision
12. [Cross-Synthesis Review Prompt](#cross-synthesis-review-prompt-anti-sycophancy-check) ‚Äî synthesizers check each other
13. [Chief Architect Reality Sync Prompt](#chief-architect-reality-sync-prompt) ‚Äî after ship
14. [Context Refresh Prompt](#context-refresh-prompt) ‚Äî when sensing drift

### Variables Reference

Variables are provided in [session-variables.md](session-variables.md). When generating prompts, substitute these placeholders:

| Variable | Description |
|----------|-------------|
| `{{PROJECT}}` | Project name |
| `{{VERSION}}` | Current version being built |
| `{{PREV_VERSION}}` | Previous version |
| `{{PR_URL}}` | Pull request URL |
| `{{BRANCH}}` | Git branch name |
| `{{SPEC_PATH}}` | Path to current spec |
| `{{PREV_SPEC_PATH}}` | Path to previous spec |
| `{{REVIEW_PATH}}` | Where reviews are stored |
| `{{TOOL_NAME}}` | Coding tool used |
| `{{ROUND_NUMBER}}` | Current review iteration |

---

## Roles

Each role has specific inputs and outputs. For human monitoring tasks (when to intervene, red flags), see [orchestrator-handbook.md](orchestrator-handbook.md#role-monitoring-guide).

### üéØ Strategist
**What they do:** Challenge assumptions, find holes in strategy, ensure coherent vision
**Input:** Initial strategy draft or idea
**Output:** Refined strategy with challenged assumptions, identified risks, clearer positioning

### üìã Product Architect
**What they do:** Translate strategy into structured PRD, define scope, write acceptance criteria
**Input:** Approved strategy, feature priorities
**Output:** PRD with user stories, success metrics, scope boundaries

### üèóÔ∏è Chief Architect
**What they do:** Break PRD into technical phases, choose patterns, define system boundaries
**Input:** Approved PRD
**Output:** Master implementation plan, tech stack decisions, phase breakdown
**Ongoing:** After each version ships, reviews codebase and updates Master Plan if reality drifted

### üë∑ Implementation Architect
**What they do:** Create detailed specs for a single phase/version, iterate with fellow architects until convergence
**Input:** Master plan section + previous version spec + current codebase state + session history
**Output:** Detailed implementation plan that builds on what ACTUALLY exists
**Iteration:** Reads fellow architect reviews, updates spec, flags tradeoffs for user decision

### üî® Developer (Agentic)
**What they do:** Write code, create PRs per the implementation spec
**Input:** Finalized implementation spec
**Output:** Working code, PR ready for review

### üîç Code Reviewer
**What they do:** Review PRs for bugs, security, adherence to plan
**Input:** PR diff + original implementation plan
**Output:** Approval or change requests with specific line citations

### üòà Adversarial Reviewer (CRITICAL ROLE)
**What they do:** Actively try to find problems. Prompted to reject by default.
**Input:** PR diff + implementation plan + other reviewers' approvals
**Output:** List of concerns, edge cases, security issues, OR specific attestation of what was tested

**Output Requirements:**
1. Attack Vectors Tried: Specific list (not just "I tried to break it")
2. Edge Cases Checked: Specific scenarios
3. If No Issues Found: Explain what made the code resilient

### üìù Synthesizer
**What they do:** Compress multiple review outputs into decision-ready summary
**Input:** All reviewer outputs (Code Reviewer + Adversarial Reviewer)
**Output:** One-page summary: what's approved, what's contested, what needs decision

### üêõ Debugger
**What they do:** Diagnose and fix issues when things break
**Input:** Error logs, reproduction steps, relevant code context
**Output:** Diagnosis + fix PR

### üß™ QA Analyst
**What they do:** Write test cases, validate against acceptance criteria
**Input:** PRD acceptance criteria + working build
**Output:** Test results, bug reports

---

## Workflow Phases

```
Phase 0: Strategy
    ‚Üì
Phase 1: Product Definition ‚Üí PRD.md
    ‚Üì
Phase 2: Technical Planning ‚Üí MASTER_PLAN.md
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     FOR EACH VERSION/PHASE          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Phase 3: Spec Convergence           ‚îÇ
‚îÇ     ‚Üì                               ‚îÇ
‚îÇ Phase 4: Development                ‚îÇ
‚îÇ     ‚Üì                               ‚îÇ
‚îÇ Phase 5: Code Review + Iteration    ‚îÇ
‚îÇ     ‚Üì                               ‚îÇ
‚îÇ Phase 6: Validation                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
Phase 7: Post-Ship (Reality Sync)
```

### Spec Convergence Loop (Phase 3)

```
Implementation Architect writes spec
            ‚îÇ
            ‚ñº
    Fellow Architects review (initial)
            ‚îÇ
            ‚ñº
    Implementation Architect responds, updates spec
            ‚îÇ
            ‚ñº
    User reviews flagged tradeoffs, decides
            ‚îÇ
            ‚ñº
    Fellow Architects re-review ‚óÑ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ                       ‚îÇ
            ‚ñº                       ‚îÇ
      All CONVERGED? ‚îÄ‚îÄNo‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
           Yes
            ‚ñº
      Spec Finalization
            ‚îÇ
            ‚ñº
      Ready for Developer
```

### Code Review Loop (Phase 5)

```
Developer creates PR
            ‚îÇ
            ‚ñº
    Code Reviewer + Adversarial Reviewer
            ‚îÇ
            ‚ñº
    Synthesizers compress for decision
            ‚îÇ
            ‚ñº
    User decides: APPROVE or REQUEST CHANGES
            ‚îÇ
            ‚ñº (if REQUEST CHANGES)
    Developer responds, pushes fixes
            ‚îÇ
            ‚ñº
    Reviewers re-review ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ                       ‚îÇ
            ‚ñº                       ‚îÇ
      APPROVE? ‚îÄ‚îÄ‚îÄ‚îÄNo‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
           Yes
            ‚ñº
         MERGE
```

---

## Anti-Sycophancy Patterns

LLMs default to agreeing. These patterns force independent thinking.

### Model-Specific Depth Triggers

| Model | Depth Trigger | Disagreement Permission |
|-------|---------------|------------------------|
| **Claude (Opus)** | "ultrathink" | "keep your integrity" |
| **Claude (Sonnet)** | "think carefully about this" | "challenge my assumptions" |
| **GPT-4 / GPT-4 Turbo** | "think step by step, be thorough" | "tell me what's wrong with this" |
| **Gemini** | "take your time, analyze deeply" | "what would you change" |

### Key Phrases That Force Divergence

| Pattern | Why It Works |
|---------|-------------|
| "What would you have done differently?" | Can't rubber-stamp‚Äîmust propose alternatives |
| "Keep your integrity" | Permission to disagree, even if other models approved |
| "Be open-minded while keeping your integrity" | Balance: consider others, but don't just conform |
| "ultrathink" | Triggers extended reasoning (Claude), prevents pattern-matching |

### Anti-Sycophancy Phrase Upgrades

| Weak Version | Strong Version |
|--------------|----------------|
| "Review this code" | "Find three things wrong with this code. If you can't find three, explain why each obvious concern doesn't apply." |
| "Is this approach good?" | "Argue against this approach. Then argue for it. Then tell me which argument is stronger." |
| "Any feedback?" | "What would you change if you were starting fresh? What would you keep exactly as-is and why?" |
| "Please review" | "Review this as if your reputation depends on finding issues others missed." |
| "What do you think?" | "Take a position. I want YOUR view, not a balanced summary of options." |

### Detecting Sycophantic Responses

**Red Flags:**
- Starts with "That's a great approach" or "You're absolutely right"
- Review is shorter than the artifact being reviewed
- No specific suggestions for changes
- Says "I agree" without explaining WHY
- Approves both options when asked to choose one

**Test Questions:**
1. "What would you have done differently if you started from scratch?"
   - Sycophantic: "I would have done it very similarly to what you have."
   - Genuine: "I would have [specific alternative] because [reasoning]."

2. "What's the strongest argument AGAINST this approach?"
   - Sycophantic: "There aren't many strong arguments against it."
   - Genuine: "[Specific concern] because [reasoning]."

---

## Prompts Library

### Strategist Prompt
```
You are a strategic advisor. Your job is to challenge assumptions and find holes.

I'm building {{PROJECT}}. Here's my current thinking:
[paste strategy draft]

Ultrathink and evaluate:
1. What assumptions am I making that might be wrong?
2. What risks am I not seeing?
3. Who else has tried this? What can we learn from them?
4. What's the strongest argument against this strategy?
5. If this fails, what will be the most likely reason?

Don't validate‚Äîchallenge. I need you to find the holes before the market does.
```

### Product Architect Prompt
```
You are a product architect creating a PRD for {{VERSION}}.

Strategy: [paste STRATEGY.md or relevant section]
Priorities: [feature priorities]

Create a PRD that includes:
1. **Problem Statement**: What user problem are we solving?
2. **User Stories**: As a [user], I want [goal], so that [benefit]
3. **Acceptance Criteria**: Specific, testable conditions for each feature
4. **Scope Boundaries**: What's IN and what's explicitly OUT
5. **Success Metrics**: How we'll know this worked
6. **Dependencies**: What must exist before this can be built

Be specific. Vague acceptance criteria lead to vague implementations.
```

### Chief Architect Prompt
```
You are the Chief Architect creating a master implementation plan.

PRD: [paste PRD.md]

Create a technical plan that includes:
1. **System Architecture**: High-level components and their relationships
2. **Tech Stack Decisions**: What technologies and why
3. **Phase Breakdown**: Split into implementable versions (v1, v2, v3...)
4. **Phase Dependencies**: What must be built before what
5. **Risk Areas**: Where are the technical unknowns?

For each phase/version, define:
- What's included
- What's the deliverable
- What's the acceptance gate

Be opinionated. Make decisions, don't list options.
```

### Implementation Architect Prompt
```
You are a technical lead writing an implementation spec for {{VERSION}}.

## Your Inputs
- Master Plan: [paste relevant section of MASTER_PLAN.md]
- Previous version spec: {{PREV_SPEC_PATH}}
- Session logs: .ontos-internal/session_logs/ (decisions made previously)
- Current codebase: Review the repo to see what actually exists

## Your Job

### 1. Reality Check (BEFORE writing anything)
- What does the codebase ACTUALLY look like right now?
- How does it differ from what the Master Plan says?
- What constraints exist from previous implementation decisions?

### 2. Assumptions (REQUIRED SECTION)
List the key assumptions you're making about:
- The existing codebase (what exists, what doesn't)
- The runtime environment (Python version, available tools)
- User behavior (how they'll invoke this, what input they'll provide)
- External dependencies (network, filesystem, git)

For each assumption: What breaks if you're wrong? How to verify or handle failure?

### 3. Detailed Spec
Include:
- **File Structure:** Exact files to create/modify (relative to repo root)
- **API Contracts:** Request/response examples, not just descriptions
- **Data Models:** Field names, types, constraints, defaults
- **Implementation Order:** Numbered steps, with dependencies noted
- **Error Handling:** What errors can occur, how to handle each

### 4. Test Strategy (REQUIRED SECTION)
Specify:
- Unit tests: Which functions, which edge cases
- Integration tests: Which workflows
- Acceptance tests: How to verify the feature works end-to-end

### 5. Developer Instructions (REQUIRED SECTION)
Step-by-step for the coding agent:
1. First, create...
2. Then implement...
3. Write tests for...
4. Before creating PR, verify...

## Critical Rules
- Be SPECIFIC. If a developer would need to make a judgment call, you haven't specified enough.
- If reality differs from Master Plan, flag the deviation and recommend: (a) refactor to match plan, (b) update plan to match reality, or (c) accept drift.
- Do not defer decisions to the Developer. Make them here.
```

### Implementation Plan Review Prompt (Fellow Architects)
```
Implementation Architect has written an implementation plan for {{VERSION}}.

The plan is at: {{SPEC_PATH}}

You are a fellow architect reviewing this plan. Ultrathink and evaluate:

1. **Reality Check**: Does this plan account for the CURRENT codebase, or does it 
   assume things that don't exist / ignore things that do?

2. **Assumption Audit**: Review the Assumptions section.
   - Are they all valid?
   - What assumptions are MISSING that should be listed?
   - Is the impact assessment correct for each?

3. **Completeness**: Will a coding agent have all info needed, or will it have to guess?

4. **Correctness**: Does this actually solve what the PRD requires?

5. **Continuity**: Does this build sensibly on {{PREV_VERSION}}, or does it ignore 
   decisions/constraints from previous implementations?

6. **Risk Assessment**:
   - What's the worst-case scenario if this ships as-is?
   - What's the most LIKELY failure mode?
   - What would you add to prevent these?

7. **What would you have done differently?**

Be specific. If you'd change the API design, show your alternative.
If you'd use a different pattern, explain why.

Don't just validate‚Äîtake a position. I need your independent technical judgment.

Output:
## Assessment
[Overall: Ready / Needs Revision / Major Rework]

## Assumption Audit
[Valid / Missing / Incorrect assumptions]

## Reality Alignment
[Does this match what actually exists in the codebase?]

## Risk Assessment
- Worst case: [X]
- Most likely failure: [Y]
- Prevention: [Z]

## Ambiguities That Will Cause Problems
[Places where the coding agent will have to guess]

## What I Would Do Differently
[Your alternative approach, with rationale]

## Specific Feedback
[Line-by-line or section-by-section comments]
```

### Implementation Architect Response to Review Prompt
```
Fellow architects have reviewed your implementation plan for {{VERSION}}.

Their reviews are in: {{REVIEW_PATH}}

Read all reviews carefully. For each piece of feedback:

1. **If you agree**: Update the spec to address it
2. **If you disagree**: Write a brief response explaining why your approach is better
3. **If it's a tradeoff**: Note both options and flag for user decision

After addressing all feedback:
- Save the updated spec to {{SPEC_PATH}}
- Create a changelog at the top of the spec summarizing what changed:
  ```
  ## Revision History
  - v2: Addressed architect reviews (YYYY-MM-DD)
    - Changed X per [Architect A] feedback
    - Kept Y despite [Architect B] suggestion because [reason]
    - FLAG FOR DECISION: [tradeoff that needs user input]
  ```

Do NOT:
- Ignore feedback without addressing it
- Make the spec vague to avoid conflict
- Remove detail to "simplify" unless the detail was actually wrong

The goal is a spec so clear that a coding agent won't need to make judgment calls.
```

### Fellow Architects Re-Review Prompt
```
Implementation Architect has addressed your feedback on the implementation plan for {{VERSION}}.

Updated spec: {{SPEC_PATH}}
(Check the Revision History at the top to see what changed)

Review the updates. For each piece of your original feedback:

1. **Addressed adequately**: Note it's resolved
2. **Not addressed / disagree with response**: Explain why, be specific
3. **New concerns**: If the changes introduced new issues, flag them

Output:
## Resolved
[List which of your concerns are now addressed]

## Still Outstanding
[List concerns that weren't adequately addressed, with specifics]

## New Concerns
[Any issues introduced by the changes]

## Verdict
CONVERGED (ready for development) / NEEDS ANOTHER ROUND (with clear asks)

Be open-minded while keeping your integrity. If the Implementation Architect 
made a reasonable case for their approach, acknowledge it even if you'd have 
done it differently.
```

### Implementation Architect Iteration Prompt (Multiple Review Rounds)
```
This is round {{ROUND_NUMBER}} of spec review for {{VERSION}}.

Previous feedback has been addressed. Fellow architects have left new comments.

Read the NEW feedback only. Address it following the same rules:
- Agree ‚Üí update spec
- Disagree ‚Üí explain why
- Tradeoff ‚Üí flag for user decision

Update the revision history. Focus on the delta.

We're aiming for convergence‚Äîall architects agree this spec is ready.
If you're still getting fundamental pushback, the scope may be too big. Flag that.
```

### Spec Finalization Prompt (After Convergence)
```
All fellow architects have agreed the implementation plan for {{VERSION}} is ready.

The spec at {{SPEC_PATH}} has gone through multiple rounds of review and may have 
accumulated revision history, resolved discussions, and deferred items.

Clean it up for the Developer:

1. **Remove cruft**: Delete revision history, resolved debates, "FLAG FOR DECISION" markers
2. **Consolidate**: If decisions were made in back-and-forth, state the final decision only
3. **Clarify order**: Make the step-by-step implementation order crystal clear
4. **Add agentic instructions**: Include specific guidance for the coding agent:
   - Which files to create first
   - Which tests to write
   - When to commit (after each component? all at once?)
   - What to include in the PR description

Output a clean, final version of the spec with a new section:

## Developer Instructions
[Step-by-step instructions for the coding agent]
1. First, create...
2. Then implement...
3. Write tests for...
4. Before creating PR, verify...

The Developer reading this should have ZERO questions. If they'd need to guess, 
you haven't been specific enough.

Save to: {{SPEC_PATH}} (overwrite the reviewed version)
```

### Development Initiation Prompt
```
You are the Developer for {{VERSION}}.

Your instructions:
- Implementation Spec: {{SPEC_PATH}}
- Branch: {{BRANCH}}
- Target: Create a PR when complete

Read the Implementation Spec carefully, especially the "Developer Instructions" section.

Execute the implementation:
1. Follow the step-by-step order exactly
2. Do not skip steps or reorder without reason
3. If something is ambiguous, STOP and ask‚Äîdon't guess
4. Commit incrementally as specified in the instructions
5. Write tests as specified before creating PR

When complete:
- Create a PR to the main branch
- Title: "{{VERSION}}: [brief description]"
- Description: Summarize what was implemented, reference the spec
- Request review

CRITICAL: Implement ONLY what's specified. If you think something should be 
added or improved, STOP and ask. Do not make judgment calls.

Do NOT:
- Add features not in the spec (scope creep)
- Skip tests to save time
- Make architectural decisions‚Äîthose were already made
- Merge your own PR
```

### Code Reviewer Prompt
```
{{TOOL_NAME}} has implemented {{VERSION}} per the implementation spec.

PR: {{PR_URL}}
Spec: {{SPEC_PATH}}

Ultrathink and review this PR.

## Required Evaluation

### 1. Spec Compliance
For each spec section, verify implementation:
- [Section X.X] ‚úì Implemented correctly
- [Section X.X] ‚ö† Deviation: [explain]
- [Section X.X] ‚úó Missing

### 2. Test Coverage
- Are edge cases from the spec tested?
- Are error conditions tested?
- Could a test pass while the feature is broken?
- What's NOT tested that should be?

### 3. Error Handling
- Are all error paths handled?
- Are error messages useful?
- Is there appropriate logging for debugging?

### 4. Risk Assessment
- What could break in production that works in test?
- What assumption is the code making that might be wrong?

### 5. What I Would Do Differently
- Take a position. Don't just validate.
- Show alternatives if you have them.

## Output Format

## Spec Compliance
[Checklist with section references]

## Test Coverage Assessment
- Coverage: [Good/Adequate/Insufficient]
- Missing tests: [list]

## Issues Found
[Severity: Critical/Major/Minor]

## Risk Assessment
[What could go wrong]

## What I Would Do Differently
[Specific alternatives with rationale]

## Recommendation
APPROVE / REQUEST CHANGES / BLOCK
[With specific reasoning]
```

### Developer Response to Review Prompt
```
Fellow architects have reviewed your PR: {{PR_URL}}

Their feedback is in the PR comments. Read all comments carefully.

For each piece of feedback:
1. If you agree: implement the fix
2. If you disagree: explain why in a reply comment, then wait for user decision
3. If you're unsure: ask for clarification in a reply comment

After addressing all feedback:
- Push your changes to the same branch
- Summarize what you changed in a comment

Do NOT:
- Ignore feedback without addressing it
- Make changes beyond what was requested
- Close or merge the PR yourself

Implementation Spec (for reference): {{SPEC_PATH}}
```

### Developer Iteration Prompt (Multiple Review Rounds)
```
This is round {{ROUND_NUMBER}} of review for {{PR_URL}}.

Previous round feedback has been addressed. New comments have been added.

Read the NEW comments only (since your last push). Address them following the same rules:
- Agree ‚Üí implement
- Disagree ‚Üí reply with reasoning
- Unsure ‚Üí ask for clarification

Focus on the delta. Don't re-review old resolved threads.

Push changes when done and summarize what changed.
```

### Code Reviewer Re-Review Prompt
```
Developer has addressed your feedback on PR: {{PR_URL}}

New commits have been pushed. Review the changes.

For each piece of your original feedback:

1. **Addressed adequately**: Resolve the comment thread
2. **Not addressed / disagree with fix**: Reply explaining why, be specific
3. **New concerns**: If the changes introduced new issues, comment on them

Focus on the DELTA‚Äînew commits since your last review. Don't re-review unchanged code.

Output:
## Resolved
[Which of your concerns are now addressed]

## Still Outstanding  
[Concerns that weren't adequately fixed]

## New Concerns
[Any issues introduced by the changes]

## Verdict
APPROVE / REQUEST CHANGES (with clear asks) / BLOCK (with reasons)

Implementation Spec (for reference): {{SPEC_PATH}}
```

### Adversarial Reviewer Prompt
```
You are a security-minded engineer whose job is to FIND PROBLEMS in {{VERSION}}.

Your default stance is to reject. You are looking for reasons this code should NOT be merged.

PR: {{PR_URL}}
Spec: {{SPEC_PATH}}
Note: Other reviewers may have approved. Be skeptical.

## Attack Vectors to Try (REQUIRED ‚Äî list what you actually tested)

**Input Validation:**
- Null/undefined input
- Empty strings
- Extremely long input
- Special characters (SQL injection, XSS)
- Boundary values (0, -1, MAX_INT)

**State & Concurrency:**
- Race conditions
- State corruption from partial failures
- Resource leaks (files, connections)

**External Dependencies:**
- Network timeout/failure
- Disk full/permission denied
- Dependency returns unexpected data

**Security:**
- Authentication bypass
- Authorization bypass
- Information leakage in errors
- Secrets in logs

## Output Format (REQUIRED ‚Äî vague responses will be rejected)

## Attack Vectors Tested
[For EACH category above, specify what you tried and what happened]
Example: "SQL injection in name field: Tried `'; DROP TABLE users; --` ‚Äî properly escaped, no vulnerability"

## Issues Found
[Severity with exploitation scenario]
- CRITICAL: [X] ‚Äî Attacker could [Y] by doing [Z]
- MAJOR: [X] ‚Äî Edge case [Y] causes [Z]

## Edge Cases Not Handled
[Specific scenarios with reproduction steps]

## Questions Other Reviewers Should Have Asked
[Gaps in their review]

## Verdict
BLOCK (with specific requirements to unblock)
‚Äî or ‚Äî
RELUCTANT APPROVE: "I tested [X, Y, Z]. The code handles them because [specific defenses]. I could not find a way to break this."

NOT ACCEPTABLE:
- "Looks good"
- "I tried to break it and couldn't" (without specifics)
- Any response under 200 words
```

### Synthesizer Prompt
```
You are preparing a decision summary for a product owner who will approve or reject this merge.

They will NOT read the raw reviews. If you miss something, they'll never see it.

## Inputs
[Paste all reviewer outputs]

## Critical Rules
1. If reviewers DISAGREE, flag prominently ‚Äî do not smooth over conflicts
2. If ANY reviewer found a security issue, it goes at the TOP
3. Compress, but don't lose decision-changing information
4. Assess reviewer quality ‚Äî flag shallow reviews

## Output Format

## TL;DR
[One sentence: Ready to merge? Yes / No / Conditional]

## Reviewer Quality Assessment
[Did any reviewer seem to rush? Give generic feedback? Miss obvious things?]
- Code Reviewer: [Thorough/Adequate/Shallow]
- Adversarial Reviewer: [Thorough/Adequate/Shallow]
- Evidence: [Why you rated them this way]

## Critical Issues (if any)
[Security issues first, then blockers]

## Reviewer Consensus
[What all reviewers agreed on]

## Conflicts Between Reviewers
[Where reviewers disagreed ‚Äî THIS SECTION IS REQUIRED even if empty]

## Open Questions for Your Decision
[Anything that needs human judgment]

## My Assessment
[Your own view ‚Äî not just synthesis of others]
Is this ready to merge? What's your confidence level (High/Medium/Low)?

## Recommendation
MERGE / REQUEST CHANGES (specify what) / BLOCK (specify why)
```

### Cross-Synthesis Review Prompt (Anti-Sycophancy Check)
```
Claude, Gemini, and GPT, acting as fellow architects, have each produced 
synthesis summaries of the code reviews for {{VERSION}}.

Their syntheses are in: {{REVIEW_PATH}}

Ultrathink and review their syntheses. Consider:
- What did the other synthesizers miss or compress away?
- Where do they disagree, and who's more right?
- What would change the product owner's decision that got lost?

Be open-minded while keeping your integrity. Don't defer to consensus if you 
think the consensus is wrong.

Output:
## Key Disagreements Between Syntheses
[What they saw differently]

## Information Lost in Compression
[Things in raw reviews that all syntheses dropped but matter]

## My Independent Assessment
[Your own synthesis, informed by but not deferring to the others]
```

### Chief Architect Reality Sync Prompt
```
{{VERSION}} has shipped and merged.

Your job: Update the Master Plan to reflect reality.

## Your Inputs
- Current Master Plan: MASTER_PLAN.md
- What was supposed to be built: {{SPEC_PATH}}
- What was actually merged: Review the codebase

## Your Tasks

1. **Reality Check**
   - Does the codebase match what the Master Plan says should exist?
   - Are there deviations (intentional pivots vs. accidental drift)?

2. **Update Master Plan**
   - Mark {{VERSION}} as complete
   - Update any sections that no longer match reality
   - Note any architectural decisions that emerged during implementation

3. **Implications for Next Version**
   - Does anything in the plan for future versions need to change?
   - Are there new constraints from what was just built?

## Output
Updated MASTER_PLAN.md with:
- {{VERSION}} marked complete
- Reality-aligned descriptions
- Notes on any drift and whether it was intentional
- Updated guidance for upcoming versions
```

### Context Refresh Prompt
Use when sensing the model is losing track of decisions or context.

```
Let's verify alignment before continuing.

Please summarize:
1. **Decisions made this session** (bullet points, be specific)
2. **Still open/unresolved** (what haven't we decided yet?)
3. **Immediate next action** (what are we about to do?)

I'll verify this matches my understanding.
```

**Expected response format:**
```
## Session Summary

### Decided
- Config merge uses read_user_config() at install time
- --strict flag fails instead of silent fallback
- filelock library for cross-platform locking

### Still Open
- Lock file location (.ontos.lock vs .ontos/.lock)
- Whether --strict should be default in v4.0

### Next Action
- Write the config parsing section of the spec
```

---

## Artifact Templates

### STRATEGY.md Structure
```markdown
# Strategy: [Project Name]

## Problem Statement
[What user problem are we solving?]

## Market Thesis
[Why now? Why us?]

## Core Insight
[What do we believe that others don't?]

## Success Criteria
[How will we know this worked?]

## Risks & Mitigations
[What could go wrong? How do we prevent it?]
```

### PRD.md Structure
```markdown
# PRD: [Feature/Version Name]

## Overview
[One paragraph summary]

## User Stories
- As a [user], I want [goal], so that [benefit]

## Acceptance Criteria
[Specific, testable conditions]

## Scope
### In Scope
[What's included]

### Out of Scope
[What's explicitly excluded]

## Success Metrics
[How we'll measure success]
```

### MASTER_PLAN.md Structure
```markdown
# Master Implementation Plan

## Architecture Overview
[System diagram, major components]

## Tech Stack
[Technologies chosen and why]

## Version Roadmap

### v1.0 ‚Äî [Name]
- Status: [Planned/In Progress/Complete]
- Features: [list]
- Acceptance gate: [criteria]

### v2.0 ‚Äî [Name]
- Status: [Planned]
- Features: [list]
- Dependencies: [what must exist first]
```

### VERSION_X_SPEC.md Structure
```markdown
# Implementation Spec: {{VERSION}}

## Assumptions
[Required table of assumptions]

## File Structure
[Exact files to create/modify]

## Implementation Details
[API contracts, data models, etc.]

## Test Strategy
[Unit, integration, acceptance tests]

## Developer Instructions
[Step-by-step for coding agent]
```

---

*This document is for LLMs. For human reference, see [orchestrator-handbook.md](orchestrator-handbook.md). For session variables, see [session-variables.md](session-variables.md).*
